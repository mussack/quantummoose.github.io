---
layout: post
title: "Partial Least Squares Regression"
date: 2017-07-08
categories: math
---

I've been using [partial least squares](https://en.wikipedia.org/wiki/Partial_least_squares_regression) analysis for a while now, but still learning about it. The core idea of PLS is really not complicated at all, but using it appropriately and fully understanding the model takes work.

What's the core idea? In short, PLS combines both principle components analysis and standard regression - instead of simply regressing Y against X, you find the best lower dimensions {% m %} Y_{low} {% em %} and {% m %} X_{low} {% em %} such that {% m %} Y_{low} {% em %} regresses against {% m %} X_{low} {% em %}. The code for running PLS in MATLAB is very simple, since there's a built in [plsregress](https://www.mathworks.com/help/stats/plsregress.html) function, but it's a bit obtuse{% sidenote "zero" "Though this might be due to my own background" %}. I'm not going to go through [PCA](http://setosa.io/ev/principal-component-analysis/) and [regression](http://setosa.io/ev/ordinary-least-squares-regression/) and just assuming we know those things.

For regression in general, we fit the equation {% m %} Y = \beta \times X + \epsilon {% em %} where we want to predict Y from X, using the {% m %} \beta {% em %} regression weights or coefficients. {% m %} \epsilon {% em %} is just the error term here. So let's assume we want to predict some behavioral response, like reaction time, from some neural signal{% sidenote "two" "Assume all the values are centered." %}, so reaction time = Y {% sidenote "three" "Y is a *trial* x 1 vector" %} and brain signals = X {% sidenote "four" "X is a *trial* by *features* matrix, where features are just number of signals we have on each trial, like average voxel signal" %}. However, we measure a large amount of different features that may or may not be important. Presumably only some of them are.

First, let's generate some fake data in MATLAB:

``` matlab
% Generating fake data:
ntrials = 300;
% Only the hidden state determines response:
HiddenState = randn(1, ntrials);
Response = HiddenState * 5 + randn(1,ntrials);
plot(HiddenState, Response, '.')

% Now unrelated noise states:
OtherStates = randn(9, ntrials);
% Project those states into a higher dimension.
linproj = randn(10, 30);
XObs = [HiddenState', OtherStates'] * linproj;
```
{% marginfigure "one" "assets/img/pls/pls1.png" "plot(HiddenState, Response, '.')" %}

So what did I do here? I created some fake regression data, where our response is predicted from the Hidden State. However, I combined the hidden state with other random states that are unimportant for prediction. ``` linproj ``` is a matrix that basically puts your 10 states (combined hidden and noise) into a higher level space (30 dimensions). This kinda replicates what often happens to our measurement. We measure a bunch of signals that are highly correlated, with only a few actually important changing states. Now what happens if we go ahead and perform regression, regressing the 30 features of Xobs against the 1 Response?
``` matlab
>>> beta = XObs\Response'; % Solve simple regression in matlab.
Warning: Rank deficient, rank = 10, tol =  1.775230e-11.
```
So MATLAB already figured out that most of the dimensions aren't predictive. We accidentally got back we only have 10 states. Our regression here actually works pretty well (in terms of fit), but it's really overfitting all the noisy dimensions.

What if most of the features in X are either redundant or not really important for predicting? Well we could go ahead and perform PCA on the XObs to find what dimensions actually are important, {% m %} XObs = Xlow \times U_L {% em %} where we only choose L components from the projection matrix U. In MATLAB we can do this by performing SVD on the covariance matrix of the features.
``` matlab
CObs = cov(XObs);
[U,S,V] = svd(CObs);
% Find percent variance explained:
cumsum(diag(S)/sum(diag(S)))
```
We find that we can explain over 80% of the variance with one component, and only need 10 components to explain everything. So we could go ahead and use those new lower dimensions for regression:
``` matlab
Xpca_scores = XObs * U(:,1:10);
beta2 = Xpca_scores\Response';
```
Again, if we only cared about fit here we do a good job, but again there's a worry of overfit. There's also another worry: if this was bad at predicting how do we assign blame? Which was the problem, the PCA or the regression? Now what if we wanted to *use* those hidden states; did we grab the right hidden states? If we wanted to somehow use the regression *with* the PCA to find the single hidden state that's important at predicting the response, then we'd have to combine these ideas.

Again, that's what PLS does. Partial Least Squares is so called when compared with Least Squares method for regression{% sidenote "five", "Interestingly, the co-creater of PLS, Svante Wold, <a href=\"http://www.sciencedirect.com/science/article/pii/S0169743901001551\">prefers the name</a> *projection to latent structures*, which is much more explanatory." %}. PCA only minimizes reconstruction error, while regression minimizes least squares fit error. PLS tries to find the lower dimension that best predicts the response Y. It's a type of *directed dimensionality reduction*. In MATLAB, it's simple:
``` matlab
ncomp = 1; % Choose the number of components.
[Xlow, Ylow, Xscores, Yscores, beta] = plsregress(Xobs,Response',ncomp);
```
{% marginfigure "two" "assets/img/pls/pls2.png" "plot(Xscores, HiddenState, '.'). Note we don't completely recover the original states" %}
Note that PLS can deal with multiple Y dimensions as well, fitting a lower Y state (here Ylow, which we ignore). The scores are the projected lower dimension values, while the Xlow and Ylow are the projection matrices, so {% m %} Response = Yscores \times Ylow' {% em %} and {% m %} XObs = Xscores \times Xlow' {% em %}. The beta weights here are the weights{% sidenote "six" "including intercept" %} *in the higher dimension space*, which is incredibly important. So {% m %} Response = Xobs \times beta(2:end) {% em %}, rather than being the weights in the lower dimension. You would have to project down using Xlow to get those values.

Notice we had to choose a number of components. ```plsregress``` will actually output percent variance explained and mean squared error for fits from 1 to ncomp, so we can use that to decide the number of components. Which value we use largely depends on how we use the results of the model fit. Here we care mostly if we get the score values corresponding to the hidden state, which we seem to be able to do.

For a good visualization, below we can see both the plane defined by the dimensionality reduction, and the line for the regression.
{% maincolumn "assets/img/pls/pls3.png" "From <a href=\"http://www.iasbs.ac.ir/chemistry/chemometrics/history/4th/5.pdf\">Wold, Sjostrom, and Eriksson 2001</a>" %}
All the data from X align on the plane, but follow the line (with noise). The line and components are chosen to induce a best correlation fit to the response Y.

That's a brief introduction into PLS, hopefully useful. More detailed explanations exist for the model specifications if you're interested{% sidenote "seven" "Look at Kevin Murphy's *Machine Learning: a Probabilistic Approach* for a good comparison of the generative model of PLS to other methods, including CCA and PCA" %}. It's good for me to think through it sometimes. PLS isn't the only method of performing directed dimensionality reduction, and it's also not the only way to deal with regression with high correlation in the features. For example, [canonical correlation](https://stats.stackexchange.com/questions/206587/what-is-the-connection-between-partial-least-squares-reduced-rank-regression-a) analysis is similar to PLS, but considers X and Y equivalent in that there isn't one 'predictor'. Really it depends on the type of data you have, and the type of question you're interested in.
